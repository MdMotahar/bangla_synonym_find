{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611cef99-e968-4208-9e8e-d9fe4a626dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import sys\n",
    "import shutil\n",
    "import pyonmttok\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def createFolders(args):\n",
    "    required_dirnames = [\n",
    "        \"data\",\n",
    "        \"Outputs\",\n",
    "        \"temp\",\n",
    "        \"Preprocessed\",\n",
    "        \"Reports\",\n",
    "        \"Models\"\n",
    "    ]\n",
    "\n",
    "    # do cleanup first\n",
    "    for dirname in required_dirnames[:3]:\n",
    "        if os.path.isdir(os.path.join(args.output_dir, dirname)):\n",
    "            shutil.rmtree(os.path.join(args.output_dir, dirname))\n",
    "\n",
    "    \n",
    "    if os.path.isdir(os.path.join(args.output_dir, \"Preprocessed\")):\n",
    "        shutil.rmtree(os.path.join(args.output_dir, \"Preprocessed\"))\n",
    "\n",
    "    for dirname in required_dirnames:\n",
    "        os.makedirs(os.path.join(args.output_dir, dirname), exist_ok=True)\n",
    "\n",
    "def _merge(args, data_type):\n",
    "    with open(os.path.join(args.output_dir, \"data\", f\"src-{data_type}.txt\"), 'w') as srcF, \\\n",
    "        open(os.path.join(args.output_dir, \"data\", f\"tgt-{data_type}.txt\"), 'w') as tgtF:\n",
    "        \n",
    "        for src_file in glob.glob(os.path.join(args.input_dir, \"data\", f\"*.{data_type}.{args.src_lang}\")):\n",
    "            tgt_file_prefix = src_file.rsplit(f\".{data_type}.{args.src_lang}\", 1)[0] + f\".{data_type}.{args.tgt_lang}\"\n",
    "            tgt_files = glob.glob(tgt_file_prefix + \"*\")\n",
    "\n",
    "            if tgt_files:\n",
    "                # when multiple references are present, pick the first one\n",
    "                tgt_file = tgt_files[0]\n",
    "                \n",
    "                with open(src_file) as f:\n",
    "                    for line in f:\n",
    "                        print(line.strip(), file=srcF)\n",
    "                \n",
    "                with open(tgt_file) as f:\n",
    "                    for line in f:\n",
    "                        print(line.strip(), file=tgtF)\n",
    "\n",
    "def _move(args, dataset_category):\n",
    "    for src_file in glob.glob(os.path.join(args.input_dir, \"data\", f\"*.{dataset_category}.{args.src_lang}\")):\n",
    "        tgt_file_prefix = src_file.rsplit(f\".{dataset_category}.{args.src_lang}\", 1)[0] + f\".{dataset_category}.{args.tgt_lang}\"\n",
    "        tgt_files = glob.glob(tgt_file_prefix + \"*\")\n",
    "        if tgt_files:\n",
    "            shutil.copy(\n",
    "                src_file,\n",
    "                os.path.join(\n",
    "                    args.output_dir,\n",
    "                    \"Outputs\",\n",
    "                    f\".src-{dataset_category}.txt\".join(\n",
    "                        os.path.basename(src_file).rsplit(f\".{dataset_category}.{args.src_lang}\", 1)\n",
    "                    )\n",
    "                ) \n",
    "            )\n",
    "            for tgt_file in tgt_files:\n",
    "                shutil.copy(\n",
    "                tgt_file, \n",
    "                os.path.join(\n",
    "                    args.output_dir,\n",
    "                    \"Outputs\",\n",
    "                    f\".tgt-{dataset_category}.txt\".join(\n",
    "                        os.path.basename(tgt_file).rsplit(f\".{dataset_category}.{args.tgt_lang}\", 1)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "    \n",
    "def moveRawData(args):\n",
    "    # move vocab models\n",
    "    shutil.copy(\n",
    "        os.path.join(args.input_dir, \"vocab\", f\"{args.src_lang}.model\"),\n",
    "        os.path.join(args.output_dir, \"Preprocessed\", \"srcSPM.model\")\n",
    "    )\n",
    "    shutil.copy(\n",
    "        os.path.join(args.input_dir, \"vocab\", f\"{args.tgt_lang}.model\"),\n",
    "        os.path.join(args.output_dir, \"Preprocessed\", \"tgtSPM.model\")\n",
    "    )\n",
    "\n",
    "    vocab_cmd = [\n",
    "        \"spm_export_vocab --model\",\n",
    "        os.path.join(args.output_dir, \"Preprocessed\", \"srcSPM.model\"),\n",
    "        \"| tail -n +4 >\",\n",
    "        os.path.join(args.output_dir, \"Preprocessed\", \"srcSPM.vocab\")\n",
    "    ]\n",
    "    os.system(\" \".join(vocab_cmd))\n",
    "\n",
    "    vocab_cmd = [\n",
    "        \"spm_export_vocab --model\",\n",
    "        os.path.join(args.output_dir, \"Preprocessed\", \"tgtSPM.model\"),\n",
    "        \"| tail -n +4 >\",\n",
    "        os.path.join(args.output_dir, \"Preprocessed\", \"tgtSPM.vocab\")\n",
    "    ]\n",
    "    os.system(\" \".join(vocab_cmd))\n",
    "\n",
    "    _move(args, \"test\")\n",
    "        \n",
    "def _lc(input_file):\n",
    "    lc = 0\n",
    "    with open(input_file) as f:\n",
    "        for _ in f:\n",
    "            lc += 1\n",
    "    return lc\n",
    "    \n",
    "\n",
    "def spmOperate(args, fileType, tokenize):\n",
    "    if tokenize:\n",
    "        modelName = os.path.join(args.output_dir, \"Preprocessed\", f\"{fileType}SPM.model\")\n",
    "        input_files = glob.glob(os.path.join(args.output_dir, \"Outputs\", f'*{fileType}-*'))\n",
    "\n",
    "        for input_file in input_files:\n",
    "            spm_cmd = [\n",
    "                f\"spm_encode --model=\\\"{modelName}\\\"\",\n",
    "                f\"--output_format=piece\",\n",
    "                f\"< \\\"{input_file}\\\" > \\\"{input_file}.tok\\\"\"\n",
    "            ]\n",
    "            os.system(\" \".join(spm_cmd))\n",
    "            os.remove(input_file)\n",
    "\n",
    "    else:\n",
    "        modelName = os.path.join(args.output_dir, \"Preprocessed\", f\"tgtSPM.model\")\n",
    "        for input_file in glob.glob(os.path.join(args.output_dir, \"Outputs\", f'*{fileType}-*.tok')):\n",
    "            spm_cmd = [\n",
    "                f\"spm_decode --model=\\\"{modelName}\\\"\",\n",
    "                f\"< \\\"{input_file}\\\" > \\\"{'.detok'.join(input_file.rsplit('.tok', 1))}\\\"\"\n",
    "            ]\n",
    "            os.system(\" \".join(spm_cmd))\n",
    "            os.remove(input_file)\n",
    "            post_cmd = f\"\"\"sed 's/‚ñÅ/ /g;s/  */ /g' -i \\\"{'.detok'.join(input_file.rsplit('.tok', 1))}\\\"\"\"\"\n",
    "            os.system(post_cmd)\n",
    "        \n",
    "        \n",
    "def tokenize(args):\n",
    "    spmOperate(args, 'src', tokenize=True)\n",
    "    spmOperate(args, 'tgt', tokenize=True)\n",
    "            \n",
    "def detokenize(args):        \n",
    "    spmOperate(args, 'tgt', tokenize=False)\n",
    "    spmOperate(args, 'pred', tokenize=False)\n",
    "\n",
    "def processData(args):\n",
    "    createFolders(args)\n",
    "    moveRawData(args)\n",
    "    tokenize(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c204639c-d510-48d0-b9eb-30beb70b5f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_file(output_dir\n",
    "required_dirnames = [\n",
    "        \"data\",\n",
    "        \"Outputs\",\n",
    "        \"temp\",\n",
    "        \"Preprocessed\",\n",
    "        \"Reports\",\n",
    "        \"Models\"\n",
    "    ]\n",
    "\n",
    "    # do cleanup first\n",
    "    for dirname in required_dirnames[:3]:\n",
    "        if os.path.isdir(os.path.join(args.output_dir, dirname)):\n",
    "            shutil.rmtree(os.path.join(args.output_dir, dirname))\n",
    "\n",
    "    \n",
    "    if os.path.isdir(os.path.join(args.output_dir, \"Preprocessed\")):\n",
    "        shutil.rmtree(os.path.join(args.output_dir, \"Preprocessed\"))\n",
    "\n",
    "    for dirname in required_dirnames:\n",
    "        os.makedirs(os.path.join(args.output_dir, dirname), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfba9bcc-1e50-4ba4-9583-1fad5c90fbb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T10:13:52.739829Z",
     "iopub.status.busy": "2022-09-18T10:13:52.739140Z",
     "iopub.status.idle": "2022-09-18T10:13:55.515892Z",
     "shell.execute_reply": "2022-09-18T10:13:55.514616Z",
     "shell.execute_reply.started": "2022-09-18T10:13:52.739799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.97)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a40d1d-bdf8-4648-8186-aa7c94087e81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T11:16:30.092159Z",
     "iopub.status.busy": "2022-09-18T11:16:30.091790Z",
     "iopub.status.idle": "2022-09-18T11:16:31.623554Z",
     "shell.execute_reply": "2022-09-18T11:16:31.622542Z",
     "shell.execute_reply.started": "2022-09-18T11:16:30.092090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/banglanmt/training/seq2seq/data_lite/data/test.bn /notebooks/banglanmt/training/seq2seq/data_lite/data/test.en\n"
     ]
    }
   ],
   "source": [
    "from dataProcessor_used import processData\n",
    "\n",
    "processData('/notebooks/banglanmt/training/seq2seq/data_lite',\\\n",
    "            '/notebooks/banglanmt/training/seq2seq/model_output2',\n",
    "            'bn','en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb017ef4-6a51-40ed-b406-1bf888a5ad80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T11:14:47.302971Z",
     "iopub.status.busy": "2022-09-18T11:14:47.302242Z",
     "iopub.status.idle": "2022-09-18T11:14:47.311911Z",
     "shell.execute_reply": "2022-09-18T11:14:47.310535Z",
     "shell.execute_reply.started": "2022-09-18T11:14:47.302971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/notebooks/banglanmt/training/seq2seq/model_output2/Outputs/src-test.txt'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "src_file = '/notebooks/banglanmt/training/seq2seq/data_lite/data/test.bn'\n",
    "# output_dir = '/notebooks/banglanmt/training/seq2seq/model_output2/'\n",
    "# os.makedirs(os.path.join(output_dir,'Outputs'))\n",
    "dataset_category = 'test'\n",
    "src_lang='bn'\n",
    "shutil.copy(\n",
    "                src_file,\n",
    "                os.path.join(\n",
    "                    output_dir,\n",
    "                    \"Outputs\",\n",
    "                    f\"src-{dataset_category}.txt\"\n",
    "                ) \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c7f3cbcf-170b-4f65-af89-0c16ac6b946f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-18T11:03:14.020164Z",
     "iopub.status.busy": "2022-09-18T11:03:14.019457Z",
     "iopub.status.idle": "2022-09-18T11:03:14.024955Z",
     "shell.execute_reply": "2022-09-18T11:03:14.024097Z",
     "shell.execute_reply.started": "2022-09-18T11:03:14.020135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sipc.src-test.txt\n"
     ]
    }
   ],
   "source": [
    "src_file = '/notebooks/banglanmt/training/seq2seq/hasan-etal-2020-low/data/sipc.test.bn'\n",
    "print(f\".src-{dataset_category}.txt\".join(\n",
    "                        os.path.basename(src_file).rsplit(f\".{dataset_category}.{src_lang}\", 1)\n",
    "                    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
